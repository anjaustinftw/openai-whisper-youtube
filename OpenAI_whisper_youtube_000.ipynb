{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6saDf6STJFV0MYTS+YB5P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjaustinftw/openai-whisper-youtube/blob/main/OpenAI_whisper_youtube_000.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **OpenAI :: Whisper**\n",
        "Setting Up a Google Colab Notebook to Isolate and Convert Audio Tracks in YouTube Videos, using OpenAI's whisper AI."
      ],
      "metadata": {
        "id": "kS_l7b5BUPRY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, select 'Runtime' from the text menu, and change the runtime type.\n",
        "Then select 'GPU' for hardware acceleration, and Save.\n",
        "It will reset your current runtime which is why you'll want to do this first.\n",
        "Hit CTRL+Reload in your browser to reload the runtime."
      ],
      "metadata": {
        "id": "B-LMMGHVWuvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get GPU assignment\n",
        "!nvidia-smi -L"
      ],
      "metadata": {
        "id": "yl-feHSKZyR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHz-CSFkUDyY"
      },
      "outputs": [],
      "source": [
        "# Update the server environment\n",
        "!apt update && apt upgrade -y && apt autoremove && echo \"All done.\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install jedi, setuptools-rust, and pytube\n",
        "!pip install jedi && pip install setuptools-rust && pip install pytube"
      ],
      "metadata": {
        "id": "suoHV0thV9sT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone and install whisper\n",
        "!pip install git+https://github.com/openai/whisper.git"
      ],
      "metadata": {
        "id": "BSfgTXRZZVXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import OpenAI::whisper & pytube::YouTube\n",
        "import whisper\n",
        "from pytube import YouTube"
      ],
      "metadata": {
        "id": "6lFcyIqcaTMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set model size to base\n",
        "model = whisper.load_model('base')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMw6pv2vbLG_",
        "outputId": "86781277-0e4e-48c1-b527-00cf77419be9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 139M/139M [00:02<00:00, 63.4MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, grab a link to a YouTube video to use for the following steps; downloading the video and isolating an audio track."
      ],
      "metadata": {
        "id": "5UyhwFk0cpnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set URL\n",
        "youtube_video_url = \"https://www.youtube.com/watch?v=Wc4bQxuypo0\"\n",
        "\n",
        "# Load URL into pytube\n",
        "youtube_video = YouTube(youtube_video_url)\n",
        "\n",
        "# Test by checking some of the video's metadata attributes.\n",
        "youtube_video.title"
      ],
      "metadata": {
        "id": "ZNH8wEmgdFys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print stream data from video\n",
        "for stream in youtube_video.streams:\n",
        "  print(stream)"
      ],
      "metadata": {
        "id": "kTtBBChQeBzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Isolate audio stream data from video\n",
        "streams = youtube_video.streams.filter(only_audio=True)\n",
        "for audio_stream in streams:\n",
        "    print(audio_stream)"
      ],
      "metadata": {
        "id": "HacOwE9efJtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select first audio stream\n",
        "stream = streams.first()\n",
        "stream"
      ],
      "metadata": {
        "id": "qAu3FuBRf8Eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dowload selected audio stream to '/content' folder\n",
        "stream.download(filename='PTL_audio_stream_first.mp4')"
      ],
      "metadata": {
        "id": "__aOfILwgV7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process the mp4 and store the results in 'output'."
      ],
      "metadata": {
        "id": "9tpJTKqwhXIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.transcribe(\"PTL_audio_stream_first.mp4\")"
      ],
      "metadata": {
        "id": "WOBHiwt5hfL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show text from output\n",
        "output['text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "VeWhNm5si0sb",
        "outputId": "c39e8ab7-282a-4c6b-8cf7-1183be8fcefa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Recently, OpenAI released Whisper, an automatic speech recognition system that is completely open-source. This means anyone can download this model, run it on their laptop, or on their own server, and start building speech processing applications. In fact, I'm running it right now, and you can see, as I'm speaking, I'm in near real-time transcribing everything I'm saying into my microphone. In this video, I'm going to show you how to get started with speech recognition and transcription using OpenAI Whisper. We're going to write Python code to process any piece of audio, starting with the speech from Jerome Powell here. Now, why is this interesting? Why do we care about audio and speech? Well, if you think about it, there's tons of valuable information that is embedded within audio and speech. What's been moving the markets the most this year? What's everyone talking about? They're talking about what Jerome Powell is saying in these speeches, and whether he's saying we're going to tighten rates aggressively, we're going to raise rates longer than expected, or higher than expected, or whether he says something like we're going to moderate the pace of rate increases. The wording he's using, and the language he's using is what is affecting the markets in a very short period of time. A second example of market moving data that's contained within audio is earnings calls, right? And earnings call can cause a stop to move by 20 or 30 percent in just an instant, right? And so what we want to do is know how to process this audio and convert it to text so that we can derive some meaning from it. There's even entire startups, like for instance, quarter here, is a FENTech application, and the entire business is based on providing these earnings transcriptions and making investor information available on investor relation sites. And so they've raised over $7 million. So 4.5 million plus 2.7 million earlier to build the startup. So who knows? Maybe this is worth $50 or $100 million, and using something like OpenAI whisper, which is open source, you could build a startup or a FENTech application that is very valuable. There's also tons of information that might be interesting inside of podcasts, or your favorite YouTubers channel. They're mentioning particular stocks. You might want to transcribe this, summarize it, and derive some type of insight out of this. But the most important takeaway from the tutorial is how we can begin taking inputs of one type. In this case, audio feed it into a model and have it output a data of another type that may be more useful to us. And we need to think about these inputs, like this text, this language, this audio, and images. Not just think about them as how we see them as humans, or how we hear them as humans. We need to think about how this data is understood by a computer. And so if you look here on the OpenAI code and text embeddings page, you'll see they use this image here where there's text, but behind it, there's actually a number. So we need to think about how we feed these sentences and convert them into numbers. And so these sentences or phrases and tokens can actually be represented as vectors of numbers. If you scroll down here, you'll see there's these embedding models that will eventually get to in this series. And you'll see that sentences or phrases that are represented in this vector space are actually close together if they are semantically similar. And so you see, Feline friends say, Miao is close to Miao. And then canine companions says close to wolf, both on buddies and Mou are close together here. But quarterback, there was a football is over here. And so we need to start thinking about how we can map this text over to numbers. And that's going to have useful applications in things like search, recommendations, clustering, anomaly detection, and classification, which we will get into as we go further along. Enough talking. Let's go ahead and get started with writing some Python code and using OpenAI whisper. So we're going to be doing the first couple of videos in this series inside of Google colab. So you'll be able to run this Python code right in the browser. And you're going to be able to use GPUs that are provided by Google. So they provide some amount of GPU time for free. Because I don't know what type of hardware everyone out there has. So we're going to start here. And then we're going to do a couple of videos where we build some applications in the terminal and some full stack web applications. But for right now, we're starting Google colab. And I'm going to share this that way. You don't have to type a lot of code. You can just execute this along with me and observe the output and learn in a very interactive way. So look down below. There's a link and open this up. This should be public and you should be able to follow along. So the first thing we need to do is execute this first line of code. So there's text here that describes what we're going to do. And so to get started, we're going to install open AI whisper. And so this is just a regular Python package that's installed using pip. And inside of Google colab, what you do is you just do pip install whisper. Now whisper, there's a Python package called just whisper that's not open AI whisper. And so you need to type this full path get hub.com slash whisper.get. And it's going to install open AI whisper. This dash queue makes it quieter. So it doesn't output all this these logs into your joker notebook. And so you can see right now it's installing open AI whisper. The next thing we're going to do is see if we have a GPU available. And so if you run this line, it's going to show you what GPU is available. And so what you want to do here is you want to go to runtime and go to change runtime type. And you can see a hardware accelerator. And so if this is none, you do GPU right here, right? And so if you run that, you should have access to a GPU. The next thing we're going to do is install this package called pie tube. And what that is, it's just a Python package that knows how to download and extract extract data from YouTube videos. So we're transcribing audio to text. And so to get our audio source, what we're going to do is just use a YouTube video of J pals speech. And so pie tube is what we're going to use for that. The next thing we need to do is import the packages we're going to use. So we installed whisper. We installed pie tube. So we're going to import whisper. And we're going to import this YouTube class from pie tube. So if I run this, both of those are going to be imported. The next thing you'll see in this notebook is this list of models. And so all this information you can find on the open AI website. I've extracted this image of the different model sizes available all over the place. And so there's different model sizes with whisper of varying, that variance sizes and various levels of accuracy. Now you can use the tiny model here or you can go to the base model or all the way up to the large model. So they all have different numbers of parameters. And but the large model here, even though it's the most accurate, it's also the most resource intensive. So you might not actually need it. Well, what I found in fact is that just using this base model worked just fine. And it worked really quickly. And I was able to do that near real time text transcription as I showed in the intro to the video. So what we're going to do here is choose the base model. If you want to experiment with different model sizes, you can change this string right here. So we're going to say model equals whisper.load model. And we're choosing the base model here. So if I run that, it will load the base model and we'll have this model object that we can call different methods on. Now, what is this model going to operate on? We need some input into the model. And then the model is going to output some text for us. So where do we get this audio? As I mentioned, we are using YouTube videos in this case. If you have an MP3 or a song, all kinds of stuff you can experiment with or you can record your own voice. Maybe you want to record a Zoom meeting and summarize it. There's all kinds of cool audio applications, but we're focused on finance right now. And so I'm taking this video of Jerome Powell. You can take an earnings transcript or trust some other things when you have time. And so this is this long, it's probably like 45 minute speech here. And what we do with Pytube here is we instantially this YouTube class here. And I'm just giving it the URL of a video. And if I run that, you'll have this YouTube object. And this is the instance of YouTube. And we're just passing in the URL. And from there, it's able to build this object and we'll have the title here. And we run this. You'll see watch live, Fed share, Powell discusses a Fed rate hike. It's if you look here, that should be what the tap title is right there. And this has tons of metadata about that video file. Right. And so one attribute we have on the YouTube video is streams. And so let me go ahead and insert an extra code cell just to show you. If I did a DIR YouTube video, you'll see there's tons of other information that's available. So if I did a DIR here, you'll see there's lots of other metadata. So you can get the channel ID. There's embed HTML, length, published date, rating, lots of extra information. Or maybe you want the thumbnail URL, for instance, or the the view count, right. And so what we're going to get in this case is streams. And so depending on your internet speed, YouTube is able to provide a different audio and video streams of different resolutions and varying audio quality. And so you see this is a list. You see those brackets here. This returns a list of stream objects. And some of those streams are of type audio. And some of them are of type video. In this case, we don't care about video processing processing. But in the future, we might do some machine learning for video or images. And so maybe you might be interested in processing video. We're going to focus on audio. And audio files are much smaller to download. So it'll be much quicker. But keep in mind, if you want to process a video or extract screens, you know, maybe you want to extract a chart or some code out of YouTube video, there's all kinds of cool stuff you can do. But we're going to focus on audio for now. Right. And so what I'm doing here is I'm going to loop through all the streams and just show you if I loop through this list, it looks just like that. So there's a lot of different streams of video and audio type. And the Pytube class, this YouTube class has this thing where you can filter the streams. And so I'm filtering down to only the audio streams. And if I do that, that'll just return this shorter list here of just the audio streams. And then I can call this dot first here. And that'll call like the very first stream. So we just need one stream. If you wanted a really high quality stream and a really high quality, you know, the largest model. If you wanted the most accuracy, that's what you would do. But this is a very simple transcription. There's no background noise or anything like that that we need to worry about. So we're just going to use the small model and the simplest first stream. So this 48 KBPS small runs very quickly. And so once we have that stream and it hasn't downloaded it yet. So we call stream dot download and we need to give it a file name. And so if I hit play there, what that's going to do is actually download that YouTube videos audio. And it's going to save it into a file for the Fed meeting right there. And so if I look here in my Google call up on the side, you'll see this files here. And this actually creates a file system in the cloud. And so that's saved as Fed meeting dot MP4 right there. Right. And so since this YouTube video starts with all this extra crap at the beginning, you know, it's like wait, it's like a waiting room or whatever. And it has some extra junk on the end that we don't need where the the announcers, the TV station starts talking. We want to strip that out. So we want to do some cleaning and processing of this audio data. And so what I'm going to do here is use FF MPaG. And so for those that are unfamiliar, FF MPaG is a solution for recording, converting and streaming audio and video. And this is basically a command line utility. And there's also a library for this where you can process audio and video from your command line. I think you can even import it as a Python package as well and run it that way. And so it knows how to work with all different types of streams. And so what this command does, it actually runs a command line command FF MPaG. It starts since I am aware and looked at this, I'm saying start at second 378. So it starts about six minutes and 18 seconds in. So if I go to six minute and 18 seconds, right. Six minute 18, you'll see Jerome Powell starts talking right. And we also know that it starts at one 31 PM and some seconds I already looked at this. And so we know how to timestamp this. So we're going to be training. We're going to be extracting data and we're going to know the timestamps in advance. And so what I'm doing is starting at 378 when he says good afternoon. And I'm going to stop at second 2715. And I'm going to output that as a new file. And so when I run this, this is going to extract run on the command line and we're going to have a new file called Fed meeting trimmed with just the piece of audio that we want to analyze. This is going to take a little while to run, but you should see a file here shortly. And if you don't see it, make sure you click refresh here and you'll see that file appear. All right. So that looks like it took a couple minutes to complete. But you now see I have this Fed meeting trim. MP4. So now I will have the exact audio data I want to operate on. And so all I need to do now, open a whisper is so easy to use. I load the model and I do model dot transcribe. And I pass it the name of the file, which is Fed meeting trim dot MP4. And I run it and I should get some text. Now I imported date time and recorded some timestamps because I was comparing like different models. I did some comparisons to see how long this thing took to run on different GPUs on CPU versus GPU and different model sizes. And so I wanted to make some notes on that. So this should take a two to four minutes to run depending on what you have access to. All right. So you can see that finished. So now output should contain a dictionary with a bunch of information. So if I run this for output, you'll see what this thing returns. So I scroll the top. You'll see this has tons and tons of information inside of it. And so you can see this has a seek time. Starting and and so you can see this actually chunks this up into tokens and text here. So you see text right here. That as a goal is an important step. And then you see these numbers, these tokens right here. And you see the start and end time for each of those parts of the speech. So if I scroll all the way up here, this takes a long time. I don't know if there's a shortcut for this. There's this little bar here. So let me scroll all the way to the top and you'll see this is a giant dictionary here. And so you can see text here. And this is a blob of all the text. Good afternoon, my colleagues and I all the way talks about Russian Ukraine, how much PC prices rose and tons of stuff that Jerome Powell talks about. And then there's this separate list over here called segments. So this segment's list has just like the first part. So you can see at zero seconds, it starts good afternoon, my colleagues and I committed to bring inflation down. And then you see here down the second segment, we have the tools we need to resolve it, restore press stability, so on and so forth. So we have this huge list of all the different sentences and timestamps for the start and time when Jerome Powell says these words. And so if I just get the output text by itself, that is just the text. And then I can output all of the segments just like that. And so I'm showing all the different seconds so that you can see it easily right there. And so the idea I had here just to make this interesting and make this a little bit, I think it's interesting enough as it is just transcribing speech to text. Like that's amazing. Stay the art technology right there. But just since everyone on this channel loves to apply things to trading and price data and things like that, I thought I'd go ahead and turn this into a data frame and then get data from interactive brokers and then combine those two together just so I can map them up and see how maybe one affects the other. So what we're doing here is importing pandas. And what I'm going to do is import some price data. So I'm using pandas to import some SPY. So some S&P 500 price data that's in a CSV file. So this is external data that I had to retrieve from interactive brokers. And I'm including a snippet of this. So either if you have interactive brokers, you can sign up for use on the link below or you can just copy this CSV file that I've included here for you so that you can follow along with this exercise. And so if you want to use interactive brokers, here is the code that I've included. You can run this code against trader workstation to retrieve this exact data or you can just copy this CSV file here so you can click raw and you can save this as a CSV file and load it into a colab. And so I'm going to click this upload button right here and I'm going to select that file. And you see right here I have SPY.CSV. And if you saw that little pop up, it says after you shut all this down, you're going to lose that file. So that's why I had to re-upload that. And that's why there's this error showing on screen now. But now that I have this SPY.CSV in place, I can actually run this and it should load the S&P 500 price data just like that. And if I run this, you'll see this pandas data frame. And if you'll remember earlier, I showed what time this started. And so there's some time zone conversions. I ended up doing here, but you see this 31 minute mark right here, 31 minute mark. That's shown central time, so forth. But I've already done this work for you. That's not the main point of this video, but there's the 31 minute mark all the way. And so you can see this is like 45 or 46 minutes along right now right here. And this is five second data. So what's in here is five second data. So 14, 37, 15, 20 seconds, 25 seconds, and so forth. Just because he's saying a lot of sentences in a short amount of time. And so what I'm doing here now that we have this price data frame, what I'm going to do is loop through all the seconds that OpenAI whisper gives me. And I'm going to get the nearest five seconds. So this is five second bars. Technically, you can divide this up into one second or three seconds. I forget the different time frames they give you. But I did five seconds here. And what I did is find the closest five seconds for that set of tokens. And I just stuffed it into a data frame, a pandas data frame column called text. And so I have the sentences chilling out here next to the OpenHyelo close and volume right here. And so you have this mapping of the text that was being said at that moment and what the price was. Right. And so what I also did was create a new column called percent. And so I wanted to record the percent that the price moved from open to close in that particular five second period. And so as expected, since the entire index, S&P 500, those are going to be very small percents right here. And so what I did is create one filter it down. And I just wanted to find where are all the big down moves in that 45 minute period. And so you can see there actually are some. So if I do big down moves equals spy. So the state frames called spy, I want to find where the percent went down by over a point 2%. And remember, this is just a five second period here. So moving that much in five seconds is pretty significant. And so you can see these moments at the 36 minute mark, 36 minutes and 20 seconds, 36 minutes and 25 seconds and 37 minutes. There's these pretty substantial moves. So that all occurred in a very short period of time. So we can do here. It's installed and plot this out. So I installed MPL finance, which is a Python package for plotting pandas data frames for financial data. And so we can just plot a candlestick chart or a bar chart. Whatever your choice of chart is right there. And so I'm importing this and installing it in co-lab. And then what I'm doing is telling it the date index. So telling it about this data frame and then I'm plotting that data frame from this period of time. So this 1436. So I'm getting that index from that time till this end time right there and plotting it. And you can see that's a pretty substantial fall for just a minute or two right there from spy 389 or 388 there all the way to 382 and just a very short period. So what's what's being said there or what's being said that led up to that you might ask like what caused the market to make that move. And so what I did here is kind of filter this down and look at you know zoom in on that particular time. And so if we look at the row 61 that's 1436 up there right that's row 61 you got to imagine like what said like right here isn't exactly what caused us. We want to look right before that. And so what I did was extract the rows that happened right before that leading up to that. So 1435 to 1436 right there and you can see what was said was this we anticipate ongoing increases. And so let's go to that part of the speech. So if we run this and we expand this out a bit we should be able to see what was being said at this time. So it looks like around the 42nd mark of 1435 here it says we anticipate ongoing increases in the target range for the federal fund rate will be appropriate in order to attain a stance of monetary policy sufficiently restrictive financial conditions of tighten significantly we're seeing the effects of demand in the most interest rate but it will take time however for the full effects of monetary restraint to be realized especially on inflation that's when our statement in term we will take on the count cumulative tightening at some point it will be appropriate there is significant uncertainty around that level of interest rates even so we still have some ways to go and incoming data since our last week suggests the ultimate level of interest rates will be higher than previously expected. And so there you have it that is the language that was associated with these particular movements in price and you might say Larry that's not that interesting you know that's you know I listened to the call or I listened to the the speech myself and it's obvious he's saying stuff and that was affecting the markets why do I need to write a whole Python program for this and you don't necessarily have to but we are learning to apply machine learning models and open AI whisper to audio and learning how to transcribe to text and we have succeeded in that and learned something in a very short period of time it's been like a 20 minute video and we're learning to think about audio as input and words as output and transforming data from one format to another and so that's pretty much it for this first video we're going to be exploring a series of open AI libraries and APIs and different techniques and so I hope you enjoyed learning a bit about open AI whisper and some things you can apply it to I'm applying it in other ways not just finance I this past weekend I competed in a hackathon and was using audio transcription or assembly AI to convert my YouTube videos to text because a lot of people have been wanting a text versions of tutorials from my website so that people can read what I said instead of watching me talk for like an hour hours and hours at a time so they can just quickly skim through it and search for what they need and so this library is super useful for transforming data from one format and to another so thanks a lot for watching I hope this inspires you to create some cool stuff and see you in the next video we'll learn how to get started with the open AI API and we'll start looking at gpt3 and summarization and classification and other different use cases for open AI so thanks a lot for watching see you next time bye\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tada!"
      ],
      "metadata": {
        "id": "xZo7QZzTjZO0"
      }
    }
  ]
}